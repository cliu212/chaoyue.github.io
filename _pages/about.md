---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a postdoc at [Halıcıoğlu Data Science Institute (HDSI)](https://datascience.ucsd.edu/), [UC San Diego](https://ucsd.edu/), working with [Dr. Misha Belkin](http://misha.belkin-wang.org/). I obtained my Ph.D. degree in Computer Science from [The Ohio State University](https://www.osu.edu/) in 2021, where I was advised by [Dr. Misha Belkin](http://misha.belkin-wang.org/). After that, I spend one year in Meta Pltforms Inc., as a research scientist. I also hold B.S. and M.S. degrees in physics from [Tsinghua University](https://www.tsinghua.edu.cn/en/).

**Research Interests:** My research focuses on the theoretical foundation of deep learning optimization and of its acceleration. I am enthusiastic in theoretically understanding the dynamics of neural network training, and the mechanisms behind. I believe that discovering fundamental properties of neural networks is a unique way to achieve this goal. I am also interested in improving network training by algorithmic acceleration and connections between optimization and generalization performance of neural networks.

*I will join the job market for a faculty position starting at 2024 Fall.*

News
======
* 2023/06: New paper showing that spikes in SGD training loss are catapult dynamics, with Libin Zhu, Adityanarayanan Radhakrishnan, Misha Belkin. See [*arXiv:2306.04815*](https://arxiv.org/abs/2306.04815)
* 2023/06: New paper on the large learning rate and fast convergence of SGD for wide neural networks, with Dmitriy Drusvyatskiy, Misha Belkin, Damek Davis and Yi-An Ma. See [*arXiv:2306.02601*](https://arxiv.org/abs/2306.02601)
* 2023/06: New paper studying the mechanism underlying clean-priority learning in noisy-label scenario, with Amirhesam Abedsoltan and Misha Belkin. See [*arXiv:2306.02533*](https://arxiv.org/abs/2306.02533)
* 2023/05: New paper showing the effect of ReLU non-linear activation on the NTK condition number, with Like Hui. See [*arXiv:2305.08813*](https://arxiv.org/abs/2305.08813)
* 2022/09: I am now a postdoc at the Halıcıoğlu Data Science Institute at UC San Diego.

Publications
======

**Aiming towards the minimizers: fast convergence of SGD for overparametrized problems**[\[pdf\]](https://arxiv.org/pdf/2306.02601.pdf)     
**Chaoyue Liu**, Dmitriy Drusvyatskiy, Mikhail Belkin, Damek Davis, Yi-An Ma    
arXiv:2306.02601 (In submission)

**ReLU soothes the NTK condition number and accelerates optimization for wide neural networks**[\[pdf\]](https://arxiv.org/pdf/2305.08813.pdf)    
**Chaoyue Liu**, Like Hui    
arXiv:2305.08813 (In submission)

**On Emergence of Clean-Priority Learning in Early Stopped Neural Networks**[\[pdf\]](https://arxiv.org/pdf/2306.02533.pdf)   
**Chaoyue Liu**\*, Amirhesam Abedsoltan\* and Mikhail Belkin    
arXiv:2306.02533 (In submission)

**Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning**[\[pdf\]](https://arxiv.org/pdf/2306.04815.pdf)    
Libin Zhu, **Chaoyue Liu**, Adityanarayanan Radhakrishnan, Mikhail Belkin   
arXiv:2306.04815 (In submission)

**Loss landscapes and optimization in over-parameterized non-linear systems and neural networks**[\[pdf\]](https://www.sciencedirect.com/science/article/abs/pii/S106352032100110X)   
**Chaoyue Liu**, Libin Zhu, Mikhail Belkin   
Applied and Computational Harmonic Analysis (ACHA) 2022.

**Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models**[\[pdf\]](https://openreview.net/pdf?id=CyKHoKyvgnp)   
**Chaoyue Liu**, Libin Zhu, Mikhail Belkin   
International Conference on Learning Representations (ICLR), 2022.

**Transition to linearity of general neural networks with directed acyclic graph architecture**[\[pdf\]](https://proceedings.neurips.cc/paper_files/paper/2022/file/23cf4f3fd33c2fb071fc40aee0ec2884-Paper-Conference.pdf)   
Libin Zhu, **Chaoyue Liu**, Mikhail Belkin   
Neural Information Processing Systems (NeurIPS), 2022.

**Quadratic models for understanding neural network dynamics**[\[pdf\]](https://arxiv.org/pdf/2205.11787.pdf)    
Libin Zhu, **Chaoyue Liu**, Adityanarayanan Radhakrishnan, Mikhail Belkin    
arXiv:2205.11787 (In Submission)

**Understanding and Accelerating the Optimization of Modern Machine Learning**[\[pdf\]](https://www.proquest.com/openview/bfa1255b23af1efb8bac1f54997af8e4/1?pq-origsite=gscholar&cbl=18750&diss=y)    
**Chaoyue Liu**   
Ph.D. dissertation, The Ohio State University. 2021.

**Two-Sided Wasserstein Procrustes Analysis.**[\[pdf\]](https://www.ijcai.org/proceedings/2021/0484.pdf)   
Kun Jin, **Chaoyue Liu**, Cathy Xia   
IJCAI, 2021

**On the linearity of large non-linear models: when and why the tangent kernel is constant**[\[pdf\]](https://proceedings.neurips.cc/paper_files/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf)    
**Chaoyue Liu**, Libin Zhu, Mikhail Belkin    
Neural Information Processing Systems (NeurIPS), 2020.

**Accelerating sgd with momentum for over-parameterized learning**[\[pdf\]](https://openreview.net/pdf?id=r1gixp4FPH)   
**Chaoyue Liu**, Mikhail Belkin    
International Conference on Learning Representations (ICLR), 2020.

**Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning**[\[pdf\]](https://arxiv.org/pdf/2003.00307v1.pdf)    
**Chaoyue Liu**, Libin Zhu, Mikhail Belkin   
arXiv:2003.00307

**Otda: a unsupervised optimal transport framework with discriminant analysis for keystroke inference**[\[pdf\]](https://ieeexplore.ieee.org/abstract/document/9162258)    
Kun Jin, **Chaoyue Liu**, Cathy Xia    
IEEE Conference on Communications and Network Security (CNS), 2020

**Mass: an accelerated stochastic method for over-parametrized learning**[\[pdf\]](https://arxiv.org/pdf/1810.13395v1.pdf)   
**Chaoyue Liu**, Mikhail Belkin   
arXiv:1810.13395

**Parametrized accelerated methods free of condition number**[\[pdf\]](https://arxiv.org/pdf/1802.10235.pdf)    
**Chaoyue Liu**, Mikhail Belkin   
arXiv:1802.10235

**Clustering with Bregman divergences: an asymptotic analysis**[\[pdf\]](https://proceedings.neurips.cc/paper_files/paper/2016/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf)   
**Chaoyue Liu**, Mikhail Belkin   
Neural Information Processing Systems (NeurIPS), 2016.

Services
======
Reviewer









